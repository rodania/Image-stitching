{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureDetector(image):\n",
    "    \"\"\"\n",
    "    Compute key points and feature descriptors using brisk method\n",
    "    \"\"\"\n",
    "    # define descriptor\n",
    "    descriptor = cv2.BRISK_create()\n",
    "    # get keypoints and descriptors\n",
    "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "\n",
    "    return (kps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf(des1, des2, crossCheck=False):\n",
    "    \"Create a feature Matcher using BruteForce method\"\n",
    "    \n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck).match(des1, des2)\n",
    "    return bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transformation(source, img_to_stitch, H):\n",
    "    \"\"\" align images together without cropping\n",
    "    \"\"\"\n",
    "    \n",
    "    # get image shapes\n",
    "    h1, w1 = source.shape[:2]\n",
    "    h2, w2 = img_to_stitch.shape[:2]\n",
    "    \n",
    "    # define the corner points for each image\n",
    "    corners1 = np.array([[0, 0], [0, h1], [w1, h1], [w1, 0]])\n",
    "    corners2 = np.array([[0, 0], [0, h2], [w2, h2], [w2, 0]])\n",
    "    \n",
    "    # trasnsform the obtained homography H to the corner points of the img_to_stitch\n",
    "    transH = cv2.perspectiveTransform(np.float32([corners2]), H)[0]\n",
    "    \n",
    "    # join transformed homography with the corner points of source image\n",
    "    newRect = np.concatenate((np.float32(corners1), transH), axis=0)\n",
    "    \n",
    "    # get the coordinates of the bounding rectangular of joined images\n",
    "    # by taking the min & max\n",
    "    [xmin, ymin] = np.int32(newRect.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(newRect.max(axis=0).ravel() + 0.5)\n",
    "    \n",
    "    # Compute the translation homography that will move (xmin, xmax) to (0, 0)\n",
    "    th = np.array([\n",
    "      [ 1, 0, -xmin],\n",
    "      [ 0, 1, -ymin],\n",
    "      [ 0, 0, 1]])\n",
    "    \n",
    "    # transform the homography to the new coordinates \n",
    "    newH = th.dot(H)\n",
    "    \n",
    "    # warp images together\n",
    "    warp = cv2.warpPerspective(img_to_stitch, newH, (xmax-xmin, ymax-ymin))\n",
    "    warp[-ymin:h1+ (-ymin), -xmin:w1+(-xmin)] = source\n",
    "    \n",
    "    return warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(source, img_to_stitch):\n",
    "    \"\"\" The function create panorama images by stitching two images together\n",
    "    \"\"\"\n",
    "    gray_source = cv2.cvtColor(source, cv2.COLOR_BGR2GRAY)\n",
    "    gray_img_to_stitch = cv2.cvtColor(img_to_stitch, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute key points and feature descriptors using the selected method\n",
    "    kp1, des1 = featureDetector(gray_source)\n",
    "    kp2, des2 = featureDetector(gray_img_to_stitch)\n",
    "\n",
    "    # get feature matcher using bf\n",
    "    \n",
    "    matches = bf(des1, des2, crossCheck=True)\n",
    "    # Sort the features in order of distance.\n",
    "    # The points with small distance (more similarity) are ordered first in the vector\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    print(\"Raw matches (Brute force):\", len(matches))\n",
    "        \n",
    "    # convert the keypoints to numpy arrays\n",
    "    kp1 = np.float32([kp.pt for kp in kp1])\n",
    "    kp2 = np.float32([kp.pt for kp in kp2])\n",
    "    \n",
    "    # we need 4 matched points at least to match images\n",
    "    # otherwise, there is nothing in common between images\n",
    "    if len(matches) > 4:\n",
    "\n",
    "        # construct the two sets of points\n",
    "        pt1 = np.float32([kp1[m.queryIdx] for m in matches])\n",
    "        pt2 = np.float32([kp2[m.trainIdx] for m in matches])\n",
    "\n",
    "        # estimate the homography between the sets of points\n",
    "        (H, mask) = cv2.findHomography(pt2, pt1, cv2.RANSAC, 4)\n",
    "        \n",
    "        warp = perspective_transformation(source, img_to_stitch, H)\n",
    "        \n",
    "        return warp\n",
    "\n",
    "    else:\n",
    "        return print('Images do not match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all images in the directory\n",
    "images = ['images/'+ img for img in os.listdir('images/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw matches (Brute force): 5920\n",
      "Raw matches (Brute force): 2629\n",
      "Raw matches (Brute force): 3077\n",
      "Raw matches (Brute force): 4118\n",
      "Raw matches (Brute force): 6544\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
